{"source": "2501.12420v2.pdf", "text": "# Consolidating TinyML Lifecycle with Large Language Models: Reality, Illusion, or Opportunity?\n\nGuanghan Wu _[‡]_, Sasu Tarkoma _[‡]_, Roberto Morabito _[∗‡]_\n\n_‡_ Department of Computer Science, University of Helsinki, Finland.\n_∗_ Department of Communication Systems, EURECOM, France.\n\n_**Abstract**_ **—The evolving requirements of Internet of Things**\n**(IoT) applications are driving an increasing shift toward bringing**\n**intelligence to the edge, enabling real-time insights and decision-**\n**making within resource-constrained environments. Tiny Machine**\n**Learning (TinyML) has emerged as a key enabler of this evolu-**\n**tion, facilitating the deployment of ML models on devices such as**\n**microcontrollers and embedded systems. However, the complexity**\n**of managing the TinyML lifecycle, including stages such as**\n**data processing, model optimization and conversion, and device**\n**deployment, presents significant challenges and often requires**\n**substantial human intervention. Motivated by these challenges,**\n**we began exploring whether Large Language Models (LLMs)**\n**could help automate and streamline the TinyML lifecycle. We**\n**developed a framework that leverages the natural language**\n**processing (NLP) and code generation capabilities of LLMs to**\n**reduce development time and lower the barriers to entry for**\n**TinyML deployment. Through a case study involving a computer**\n**vision classification model, we demonstrate the framework’s**\n**ability to automate key stages of the TinyML lifecycle. Our**\n**findings suggest that LLM-powered automation holds potential**\n**for improving the lifecycle development process and adapting**\n**to diverse requirements. However, while this approach shows**\n**promise, there remain obstacles and limitations, particularly in**\n**achieving fully automated solutions. This paper sheds light on**\n**both the challenges and opportunities of integrating LLMs into**\n**TinyML workflows, providing insights into the path forward for**\n**efficient, AI-assisted embedded system development.**\n_**Index Terms**_ **—TinyML, Large Language Models (LLMs), Life-**\n**cycle Automation, Embedded IoT Systems, MLOps for TinyML,**\n**Edge AI.**\n\nI. INTRODUCTION\n\nThe rapid development of Internet of Things (IoT) technologies and applications has generated a fast-growing need for the\ndevelopment and deployment of machine learning (ML) models at the edge on resource-constrained devices, a paradigm\ntypically called _TinyML_ [1]. TinyML refers to the deployment\nof machine learning models on resource-constrained devices\nsuch as microcontrollers. Unlike traditional ML, which typically runs on powerful cloud or edge servers, TinyML models\nmust operate under tight limitations in memory, compute\npower, and energy, making their development and lifecycle\nmanagement significantly more complex [2]. TinyML aims to\nprovide intelligent capabilities to small-size factor and powerefficient devices, unlocking ML-powered applications in smart\nhome automation, health, and industrial monitoring. However,\nthe lifecycle management of TinyML models presents unique\nchallenges that demand high human intervention most of the\ntime.\n\nThe ML model lifecycle includes various stages, from\ninitial development to deployment and maintenance, typically\norganized as a pipeline where each component processes\ndata and feeds its output into the next stage. This includes\ndata processing, model training and optimization, and final\ndeployment onto end devices. Managing the TinyML lifecycle\non constrained IoT devices, with their limited computing\nresources and diverse hardware configurations, adds layers\nof complexity. The landscape becomes further complicated\nby the heterogeneity of end-device hardware and software,\nthe use of different AI accelerators, and the broad range\nof device capabilities—all of which make TinyML lifecycle\nmanagement particularly challenging.\nThis complex and diverse ecosystem has boosted significant\ninterest in developing automated solutions, which is especially\npronounced in large-scale IoT deployments, where hundreds\nor thousands of edge devices may be deployed across smart\ncities, industrial networks, or low-power autonomous systems.\nIn these contexts, the efficient and scalable management of ML\nmodels becomes essential, as manual intervention is resourceintensive and often impractical.\nAt the other end of recent AI advancements, Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing, code generation, and tasks\nautomation. While LLMs require substantial computational\nresources and data, their advanced capabilities have inspired\nresearch into their potential applications within the IoT context. For example, prior studies have explored the intersection\nbetween Generative AI and IoT [3], envisioning how LLMs\ncan assist developers with code-related tasks, such as creation,\ncompletion, and debugging, to support IoT applications development. However, applying LLMs to automate complex\nlifecycle management tasks for TinyML workflows remains\nlargely unexplored.\nOur work takes an initial step toward addressing this gap\nby leveraging LLMs not only for code generation but also to\nstreamline and automate key TinyML lifecycle components,\nrepresenting an additional contribution to the intersection of\nGenerative AI and IoT. Specifically, we developed a framework that incorporates the natural language understanding and\ncode generation capabilities of GPT-4o, a leading LLM, as a\ncomponent within a broader system for automating TinyML\nlifecycle stages. The framework itself plays a critical role\nin coordinating these stages within the pipeline, managing\ninteractions with the LLM, and adapting each stage’s out\n\nputs to ensure compatibility with constrained devices. Unlike\ntraditional TinyML toolchains (e.g., TensorFlow Lite [4],\nEdge Impulse [5]) that require specialized human involvement\nacross multiple steps, our framework focuses on minimizing\ndeveloper effort by using LLMs to automate key tasks such\nas configuration generation, conversion scripting, and deployment sketch creation. Rather than replacing these toolchains,\nour framework is designed to be complementary, offering\nan automation layer particularly beneficial in hyperscale IoT\ndeployments, where managing tailored workflows across numerous heterogeneous devices can become increasingly cumbersome.\nIn this paper, we introduce the main requirements and\ncomponents of our framework and present a detailed case\nstudy involving a deep neural network-based classification\napplication on a typical constrained IoT device to evaluate\nthe effectiveness of our approach. Our empirical findings\ndemonstrate that LLM-powered automation can enhance the\nefficiency and adaptability of TinyML development, offering a\npromising approach for innovation in AI-assisted IoT systems.\nAt the same time, we recognize that current LLM technologies\nand our framework have notable limitations, highlighting that\nfull automation may still remain somewhat aspirational. Based\non our experience, we also address the challenges and opportunities identified, outlining a roadmap for future exploration.\n\nII. NAVIGATING THE TINYML LIFECYCLE\n\nWith the extension of ML models to IoT devices, TinyML\nsystems face several constraints that present three key challenges:\n\n1 Computational Resource Constraints: Edge devices, particularly microcontrollers, necessitate specialized model architectures to function within their limited processing power\nand memory capacities. TinyML target devices, such as microcontrollers, typically have kilobyte-scale memory, reduced\ncomputational unit sizes, low clock frequencies, and simplified\narchitectural features, often supporting only integer operations\n\n[2]. For instance, the Arduino Nano 33 BLE Sense, a widely\nused TinyML board, operates with a 64 MHz clock, 1 MB\nof flash memory, and 256 KB of RAM, exemplifying these\nconstraints.\n\n2 Efficiency and Performance Trade-offs: TinyML models\nmust balance energy efficiency, processing speed, and compact\nmodel size against potential reductions in accuracy due to\nlimited resources. For long autonomous operation on lowcapacity batteries, TinyML models prioritize efficiency metrics\nover raw performance. Techniques like model compression and\noptimization are applied to reduce energy consumption and\nlatency, but these often come at the cost of model precision,\nrequiring developers to carefully weigh performance tradeoffs.\n\n3 Platform Diversity and Compatibility: The significant diversity in computing units and instruction set architectures creates substantial challenges for TinyML development [4]. This\nhardware heterogeneity includes variations in specifications,\ndevelopment methodologies, and proprietary libraries, compli\n\ncating the scalability and compatibility of TinyML applications\nacross platforms [5]. Additionally, differing user requirements,\nsuch as resource and energy constraints, often necessitate the\ndevelopment of device-specific models, underscoring the need\nfor adaptable and compatible solutions.\nThese challenges highlight the need to rethink various stages\nof the TinyML lifecycle to enable the effective deployment\nof ML models on resource-constrained devices like microcontrollers [1]. Fig. 1 compares the traditional ML lifecycle\nwith a generic TinyML lifecycle. Both lifecycles consist of\nthree stages: data collection, development, and deployment &\nmonitoring. However, the TinyML lifecycle involves additional\nsteps and distinctions. For example, TinyML data is typically\ncollected via edge device peripherals, such as sensors, from\nreal or simulated production environments. During the ML\nmodel development stage, techniques such as quantization and\nmodel compression are applied to address the challenges posed\nby hardware heterogeneity and constraints specific to each\ndeployment environment. Before deployment, models must\nbe converted to formats compatible with TinyML frameworks\n(e.g., tflite for TensorFlow Lite) and compiled into forms\nthat can run on microcontrollers, often through the use of\ndedicated ML software libraries suitable for embedded ML,\nrequiring C/C++ code or a lightweight runtime to interpret\nthe model on the target device [6].\nTo sum up, TinyML development demands interdisciplinary\nexpertise spanning software, hardware, and ML. The field remains nascent, with limited benchmarks and immature tooling,\nwhich together present substantial obstacles to widespread\nTinyML implementation and optimization [5]. Second, the\ncomplexity of seamlessly managing each step of the TinyML\nlifecycle underscores the need for streamlined and automated\nmechanisms. This is where we envision that LLMs can play\na transformative role.\n\nIII. THE RISE OF LANGUAGE MODELS\n\nBuilt upon transformer architectures with self-attention\nmechanisms, LLMs owe their versatility across various\ntasks—including text generation, translation, and software\ndevelopment—to their enormous scale and the diversity\nof data on which they are trained. For example, OpenAI’s\n\ni GPT-4o model, which boasts approximately 1.8 trillion\n\nparameters across 120 layers, draws on extensive training\ndatasets spanning diverse domains, allowing it to perform\ncomplex tasks and adapt to a wide range of applications [7].\n\n**LLMs in Specialized Domains: Software Engineering and**\n**Code Generation.** These models have shown transformative\ncapabilities in software engineering, particularly in code\ngeneration and understanding. They can autonomously\nperform tasks ranging from code completion to complex\nsoftware architecture design. Advanced models like GPT4o and Codex have demonstrated proficiency across\nmultiple programming languages, often matching humanlevel performance in various coding challenges [8].\nBeyond generating simple code snippets, LLMs can\n\ni\n**Trational ML**\n**TinyML Lifecycle**\n\n**Lifecycle**\n\n**Data Collection**\n\n**Stage**\n\n**Development**\n\n**Stage**\n\n**Deployment &**\n**Monitoring Stage**\n\n**Data Collection**\n\n**Data Processing**\n\n**Model Design**\n\n**& Training**\n\n**Model Evaluation**\n\n**Deployment**\n\n**Monitoring &**\n**Model Update**\n\nFig. 1: Comparison of Traditional ML and TinyML Lifecycles. The TinyML lifecycle introduces additional steps such as\nmodel optimization, compression, and targeted IoT deployment to adapt to constrained hardware environments. Techniques\nlike quantization-aware training, pruning, and knowledge distillation are employed to ensure efficient model performance on\nresource-limited devices, highlighting the unique adaptations required for TinyML applications.\n\nhandle sophisticated programming tasks such as algorithm\nimplementation, API integration, and even translating code\nacross languages, making them highly versatile for developers.\nLLMs are also capable of understanding code structure and\nsemantics, enabling tasks such as documentation generation,\nautomated debugging and supporting the generation of\ncustomized code suited to diverse development environments.\nFor instance, they can optimize generated code to align\nwith specific system constraints, aiding in adaptation for\nheterogeneous computing environments [9]. This adaptability\nmakes LLMs valuable tools for efficient, context-sensitive\ncode generation, aligning well with the resource-constrained\nand specialized requirements of TinyML applications that we\ninvestigate in this work.\n\n**The Role of Prompt Engineering.** To fully unlock their\npotential, LLMs often rely on prompt engineering, which is\na technique designed to elicit precise and reliable outputs\nfrom these models. Crafting effective prompts enables LLMs\nto follow structured instructions, producing desired outcomes\nacross a range of complex tasks. Effective prompts typically\ninclude _role definition_, _instructions_, _context_, _input data_,\nand _output indicator_ . Core prompt engineering techniques,\nsuch as Few-Shot Prompting, Chain-of-Thought reasoning,\nand Self-Consistency prompting, enhance the accuracy and\ncontrol of LLM responses, especially for multi-step processes\n\n[10]. These approaches help to improve LLM performance in\ntasks like code generation but also address domain-specific\nchallenges by providing clear, step-by-step guidance.\n\n**LLMs and IoT: An Expanding Intersection.** Generative\nAI is increasingly shaping advancements in IoT and edge\ncomputing, bringing new capabilities to data processing,\ninteraction, sensing, and security [11]. Looking specifically at\nLLMs, as a subarea of GenAI technologies, research reveals\nseveral promising directions in this expanding intersection.\nExpectedly, LLMs are instrumental in _code generation and_\n_customization_ for IoT, enabling model adaptation across\ndiverse environments, such as federated learning, where\nmodels adjust to client-specific data and hardware [9]. In\nthe same area, it is demonstrated that advanced models like\nGPT-4o can generate code for complex embedded systems\ntasks, such as register-level drivers and power optimization\ntechniques [12]. In _personalized device interaction_, fine-tuned\nLLMs allow IoT devices to respond to natural language\ncommands, making technology more accessible by allowing\nusers to configure devices through conversational interfaces\n\n[13]. LLMs also show potential in _sensor data analysis_,\nsupporting real-time interpretation for specialized tasks\nlike gesture recognition, enhancing applications in health\nmonitoring and smart wearables [14]. In _proactive network_\n_security_, LLMs are advancing IoT security through automated\nvulnerability testing, enhancing protocol fuzzing by extracting\nprotocol information and reasoning about device responses\n\n[15]. All these applications showcase the versatility of LLMs\nwithin IoT. With this work, we open up a new area of\nexploration on how advanced AI, like LLMs, can enhance\nembedded ML.\nIV. LLMS REQUIREMENTS TOWARDS AUTOMATING THE\nTINYML LIFECYCLE.\n\nAutomating the TinyML lifecycle requires a holistic approach to leveraging LLMs that goes beyond simply generating code. To address the specific constraints and complexity of\nTinyML applications, an effective framework must incorporate\na range of tools, prompt engineering techniques, and adaptive strategies that enable LLMs to interact seamlessly with\nresource-constrained IoT environments. These requirements\ncan be grouped into key areas:\n\n_• Adaptive Prompt Engineering:_ Prompt engineering is\ncritical for eliciting reliable, task-specific outputs from\nLLMs, particularly when managing the unique requirements of TinyML tasks. Different prompt engineering\ntechniques allow the model to better understand and\nexecute multi-step processes. Furthermore, customized\nprompt templates tailored to different stages of the\nTinyML lifecycle, such as model quantization or sketch\ngeneration, streamline interactions and reduce resource\nconsumption by guiding the LLM with precise, contextspecific instructions, ensuring that outputs align with both\napplication goals and device limitations. Furthermore, as\nthe TinyML lifecycle involves multiple inter-dependent\nstages, LLMs need to ensure seamless consistency among\nthe different lifecycle pipeline stages.\n\n_• Error Handling and Iterative Refinement:_ Due to the\noften complex and context-dependent nature of TinyML\ntasks, a robust framework must include mechanisms for\ndetecting and correcting errors. Iterative prompting and\nself-consistency checks help the LLM refine its outputs,\nparticularly in stages where minor mistakes could compound into larger issues, such as model deployment or\nhardware-specific adjustments. These iterative methods\nhelp to improve output reliability and to reduce the need\nfor human intervention by enabling the system to selfcorrect when feasible.\n\n_• Tool Integration and Orchestration:_ The TinyML lifecycle requires close integration of the LLM with specialized embedded ML libraries, device-specific compilers,\nand lightweight runtimes. Effective orchestration between\nthese components and the LLM is essential for managing\nworkflows, coordinating data inputs, and ensuring that\neach output is compatible with the constrained target\nenvironment. Middleware LLMs frameworks, such as\nLangChain for prompt management and LangSmith for\nmonitoring LLM interactions, play a pivotal role in enabling these complex, multi-component interactions.\n\n_• Human-in-the-loop Interventions:_ Although the goal is to\nreduce human involvement, certain stages in the TinyML\nlifecycle may still benefit from expert oversight. A welldesigned framework can provide targeted suggestions or\nalerts for human review when complex or high-stakes\ntasks are encountered. This allows for a balanced approach, where automation handles routine processes, but\nhuman expertise is available for critical interventions.\n\n|Col1|LLM Inference<br>Provides answers<br>/generate code<br>for various tasks|Col3|\n|---|---|---|\n|<br>This dataset contains...<br>The purpose of this model...<br>The target board is...<br>I want x-bit quantization...<br> <br>Provides answers<br>/generate code<br>for various tasks<br>_ [HTTPS]_<br>_OpenAI-compatible_<br>Negotiation,<br>|<br>Provides answers<br>/generate code<br>for various tasks<br><br>|<br>Provides answers<br>/generate code<br>for various tasks<br><br>|\n|<br>This dataset contains...<br>The purpose of this model...<br>The target board is...<br>I want x-bit quantization...<br> <br>Provides answers<br>/generate code<br>for various tasks<br>_ [HTTPS]_<br>_OpenAI-compatible_<br>Negotiation,<br>|<br>Provides answers<br>/generate code<br>for various tasks<br><br>||\n|**Python**<br>**Environment**<br>**Lifecycle Middleware**<br>**     Executor**<br>**  TFLM C++ Lib**<br>Implements ML model code<br>for low-power microcontrollers<br>**arduino-cli**<br>Sketch compiler. To compile<br>**`.ino`** fle for Arduino devices<br>**Utility Libraries**<br>E.g., numpy and pandas<br>for data processing<br>**LLM Tracing**<br>Manages monitoring, tracing,<br>and storage of LLM<br>queries. E.g., LangSmith<br>**    TFLite Python Lib**<br>Handles model conversion<br>and quantization.<br>**Prompt Management**<br>**and LLM Interaction**<br>Manages prompt templates<br>and LLM interactions.<br>E.g., LangChain<br>**LLM Prompt Templates**<br>Pre-defned templates to<br>customize prompts for various<br>tasks.<br>_US_<br>_API_<br>decision making,<br>error handling...<br>**Custom Code Modules**<br>Additional Python functions<br>and methods tailored for<br>middleware components<br>integration.|**Python**<br>**Environment**<br>**Lifecycle Middleware**<br>**     Executor**<br>**  TFLM C++ Lib**<br>Implements ML model code<br>for low-power microcontrollers<br>**arduino-cli**<br>Sketch compiler. To compile<br>**`.ino`** fle for Arduino devices<br>**Utility Libraries**<br>E.g., numpy and pandas<br>for data processing<br>**LLM Tracing**<br>Manages monitoring, tracing,<br>and storage of LLM<br>queries. E.g., LangSmith<br>**    TFLite Python Lib**<br>Handles model conversion<br>and quantization.<br>**Prompt Management**<br>**and LLM Interaction**<br>Manages prompt templates<br>and LLM interactions.<br>E.g., LangChain<br>**LLM Prompt Templates**<br>Pre-defned templates to<br>customize prompts for various<br>tasks.<br>_US_<br>_API_<br>decision making,<br>error handling...<br>**Custom Code Modules**<br>Additional Python functions<br>and methods tailored for<br>middleware components<br>integration.|**Python**<br>**Environment**<br>**Lifecycle Middleware**<br>**     Executor**<br>**  TFLM C++ Lib**<br>Implements ML model code<br>for low-power microcontrollers<br>**arduino-cli**<br>Sketch compiler. To compile<br>**`.ino`** fle for Arduino devices<br>**Utility Libraries**<br>E.g., numpy and pandas<br>for data processing<br>**LLM Tracing**<br>Manages monitoring, tracing,<br>and storage of LLM<br>queries. E.g., LangSmith<br>**    TFLite Python Lib**<br>Handles model conversion<br>and quantization.<br>**Prompt Management**<br>**and LLM Interaction**<br>Manages prompt templates<br>and LLM interactions.<br>E.g., LangChain<br>**LLM Prompt Templates**<br>Pre-defned templates to<br>customize prompts for various<br>tasks.<br>_US_<br>_API_<br>decision making,<br>error handling...<br>**Custom Code Modules**<br>Additional Python functions<br>and methods tailored for<br>middleware components<br>integration.|\n|**TinyML Device**<br>**Arduino Nano 33 BLE**<br>_B/Serial_<br>**Output: Artefacts**<br>processed<br>dataset<br>converted<br>model<br>Arduino<br>sketch<br>TinyML Runtime<br>ML<br>Model<br>TFLite: TensorFlow Lite / TFLM: TensorFlow Lite Micro|**TinyML Device**<br>**Arduino Nano 33 BLE**<br>_B/Serial_<br>**Output: Artefacts**<br>processed<br>dataset<br>converted<br>model<br>Arduino<br>sketch<br>TinyML Runtime<br>ML<br>Model<br>TFLite: TensorFlow Lite / TFLM: TensorFlow Lite Micro|**TinyML Device**<br>**Arduino Nano 33 BLE**<br>_B/Serial_<br>**Output: Artefacts**<br>processed<br>dataset<br>converted<br>model<br>Arduino<br>sketch<br>TinyML Runtime<br>ML<br>Model<br>TFLite: TensorFlow Lite / TFLM: TensorFlow Lite Micro|\n|TFLite: TensorFlow Lite / TFLM: TensorFlow Lite Micro|TFLite: TensorFlow Lite / TFLM: TensorFlow Lite Micro|TFLite: TensorFlow Lite / TFLM: TensorFlow Lite Micro|\n\nFig. 2: A detailed view of the Lifecycle Middleware components, organized by function. ML Software Libraries (orange) handle model preparation, LLM Integration Components\n(green) manage interaction with OpenAI’s GPT models, Custom Code Modules (dark pink) provide additional frameworkspecific logic and user interaction, and Utility Libraries (dark\ngray) support data handling.\n\nIn summary, while LLMs offer powerful capabilities, realizing their full potential for TinyML lifecycle automation\nrequires a carefully designed ecosystem of tools and strategies.\nIn the next section, we introduce how the developed framework\ntravels in the direction of offering a practical solution for AIassisted TinyML development.\n\nV. BLUEPRINT FOR AUTOMATION\n\nFollowing the requirements outlined in the previous sections, we introduce our framework designed to integrate OpenAI’s GPT family of LLMs with TinyML tools, aimed at\nautomating key stages of the TinyML lifecycle. We present\nthe framework from two perspectives regarding the what and\nthe how, emphasizing both its structural components and\noperational workflow.\n**The ”what” — Framework Components.** As shown in\nFig. 2, there are several essential building blocks that shape\nthe ecosystem around our framework. The framework acts as\nan orchestrator, connecting human-in-the-loop input, LLM interactions, embedded ML libraries, and the hardware/software\nrequirements of target devices. Each component contributes to\nthis ecosystem, with dedicated roles such as model conversion,\n|Lifecycle Middleware|Col2|\n|---|---|\n|||\n\nFig. 3: The left panel ( _Worfklow Stages_ ) shows the core functional stages, from data processing to model conversion and sketch\ngeneration, guided by a middleware that interacts with the LLM for task-specific code generation and iterative feedback. The\nright panel ( _Workflow Logic_ ) outlines the five-step logical flow: from acquiring user inputs to iterating for output refinement,\noffering a detailed overview of the procedural steps involved in TinyML application lifecycle management.\n\nquantization, dynamic prompt generation, and error handling.\nTogether, these elements enable operational artifacts, robust\nfeedback loops, and adaptable workflows, establishing the\nframework’s technical foundation.\n**The ”how” — Process Workflow.** Illustrated in Fig. 3, it\nfocuses on the structured workflow that drives TinyML applications from user input to deployment. The Workflow Stages\n(left panel) outline the main functional steps: Data Processing,\nModel Conversion, and Sketch Generation, among others.\nThese stages are orchestrated by the Lifecycle Middleware\n(detailed in Fig. 2) that bridges user-defined goals with LLMpowered code generation. Meanwhile, the Workflow Logic\n(right panel) presents a five-step logical flow from user input\nacquisition to iterative refinement, ensuring each stage receives\nfeedback for continuous improvement and precision.\nAt the heart of the Fig. 2 framework is the _Lifecycle Middle-_\n_ware_, which integrates multiple specialized components. Custom code modules implement the core logic of the framework\nand facilitate user interactions, while ML software libraries\nprepare machine learning models for deployment by managing tasks such as conversion and quantization. Dedicated\ncomponents for LLM integration coordinate interactions with\nOpenAI’s GPT models, and utility libraries handle essential\ndata processing and manipulation tasks to ensure smooth\nworkflow transitions. Importantly, once the user provides the\ndataset, the subsequent workflow is fully automated. The\nframework leverages a set of curated prompt templates, each\ncorresponding to a different lifecycle stage (e.g., data processing, model conversion, deployment). These templates are\ndynamically adapted based on the task context and feedback\nsignals, such as errors captured during execution, allowing the\nsystem to invoke the LLM autonomously and progress through\nthe pipeline without requiring further human intervention.\nThe _Custom Code Modules_ act as the central orchestrator,\nguiding the flow of user-defined goals such as requirements,\npreferences, and specifications into the LLM environment. It\nfacilitates seamless communication between the LLM and the\n\nframework, ensuring that outputs are continuously refined and\nadapted to align with the hardware and software constraints\nof TinyML devices. These modules enable the framework\nto generate outputs that are then processed by specialized\n_embedded ML libraries_, such as TFLite, for model conversion\nand quantization, ensuring alignment with the hardware and\nsoftware constraints of the target devices. This is achieved by\ncoordinating tasks and managing the iterative exchange with\nthe LLM.\nThe LLM-related components, depicted in green in Fig.\n2, handle the interactions with OpenAI’s GPT models and\nplay a crucial role in refining outputs iteratively. _LLM Prompt_\n_Templates_ provide pre-defined structures that guide the LLM in\ngenerating context-specific responses, tailored to meet TinyML\nlifecycle needs. The _Prompt Management and LLM Interaction_\nmodule, facilitated by tools like LangChain, manages the flow\nof prompts and responses, ensuring that each step aligns with\nthe user-defined goals and device constraints. Additionally,\n_LLM Tracing_ tools, such as LangSmith, enable monitoring,\ntracing, and storage management of LLM interactions, allowing for debugging and optimization throughout the workflow.\nUltimately, the framework generates essential artifacts, including processed datasets, optimized models, and executable\nsketches that are fully compatible with TinyML devices.\nIn each lifecycle stage automated by the framework (Fig. 3\nleft panel), the workflows follow a consistent logic, though\neach stage, such as data processing, model conversion, or\nsketch generation, has specific operations and techniques. By\nadhering to a standardized workflow (Fig. 3 right panel),\neach stage begins by gathering stage-specific information. For\nexample, in data processing, the system collects information\nabout the dataset and model purpose, while in model conversion, it gathers details like dataset overview and quantization\nrequirements. Using the gathered input, the framework constructs tailored prompts specific to each stage. These prompts\nare then passed to the LLM to generate configuration settings\nor code snippets, which the framework subsequently executes\nUser\n\nFramework LLM Compiler Target Device\n\nPrevious Stages\n\nDeplotyment Stage\n\n**Full Prompt Template**\nThe template integrates high-level context and task-specific instructions to\nguide the LLM effectively.\n\n```\n### CONTEXT ###\n\"You are an expert in Tiny Machine Learning (TinyML),\nknowledgeable in workflows, tools, techniques, and best practices\nfor TinyML operations, especially on microcontrollers.\"\n\n### OBJECTIVE ###\n\"Generate accurate and efficient code for TinyML tasks, including\ndata engineering, model training, and deployment. Adapt outputs\nbased on specific hardware requirements and constraints.\"\n\n### TASK INSTRUCTIONS ###\n- **Target Goal**: \"Read the board and application specifications,\nfill parameters accurately, and avoid using generic libraries.\nReplace 'TensorFlowLite.h' with 'Arduino_TensorFlowLite.h' for\ncompatibility.\"\n- **Specification Requirements**:\n - Only output a single code block, adhering strictly to the\nprovided specifications.\n - Ensure the response is \"clear, accurate, and strictly\nfollowing the target goal.\"\n\n### ERROR HANDLING INSTRUCTIONS ###\n- \"In case of compilation or runtime errors, provide revised code\nconsidering the error context. Focus on enhancing compatibility\nwith hardware constraints (e.g., memory, processing power).\"\n- \"Avoid assumptions if specific details are missing; only proceed\nwith clearly provided parameters.\"\n\n### ITERATIVE REFINEMENT ###\n- **Loop Mechanism**: Upon failure, re-query the LLM with refined\nprompts based on previous errors, allowing the framework to\niteratively improve the generated code up to a predefined attempt\nlimit.\n\n```\n\n|Col1|Provide deployment/ model specs, application|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n||<br>~~application~~<br>requirements|<br>~~application~~<br>requirements|<br>~~application~~<br>requirements|<br>~~application~~<br>requirements|<br>~~application~~<br>requirements|<br>~~application~~<br>requirements|\n||<br>~~application~~<br>requirements|Return code for .ino sketch<br>Try compiling sketch|Return code for .ino sketch<br>Try compiling sketch|Return code for .ino sketch<br>Try compiling sketch|Return code for .ino sketch<br>Try compiling sketch|Return code for .ino sketch<br>Try compiling sketch|\n||<br>~~application~~<br>requirements||||||\n|Alte|rnative|Successful compilation<br>Deploy compiled sketch|||||\n|[els<br>[If<br>is S|[els<br>[If<br>is S|[els<br>[If<br>is S|[els<br>[If<br>is S|[els<br>[If<br>is S|[els<br>[If<br>is S|[els<br>[If<br>is S|\n|[els<br>[If<br>is S|[els<br>[If<br>is S|Unsuccessful compilation<br>Log error|||||\n|R|un Loop|t in MaxAttempts...]<br>Sends error info, query for sketch re-generation|||||\n||||||||\n|||Return re-generated code<br>Try compiling re-generated sketch|Return re-generated code<br>Try compiling re-generated sketch|Return re-generated code<br>Try compiling re-generated sketch|Return re-generated code<br>Try compiling re-generated sketch|Return re-generated code<br>Try compiling re-generated sketch|\n||||||||\n|A|lternative|Successful re-compilation<br>Deploy compiled sketch|||||\n|[<br><br>[|[<br><br>[|[<br><br>[|[<br><br>[|[<br><br>[|[<br><br>[|[<br><br>[|\n|[<br><br>[|[<br><br>[|Report compilation error|||||\n|[<br><br>[|||||||\n\nProgram ends\n\nExit\n\nFramework LLM Compiler Target Device\n\nUser\n\nFig. 4: Sequence of the Deployment Sketch Generation. The diagram illustrates the iterative process of generating and deploying\nan Arduino sketch, showing interactions between the user, framework, LLM, arduino-cli compiler, and target device, with\nbuilt-in error handling and retry mechanisms. The prompt snippet on the right highlights the structured setup used to guide\nthe LLM in generating code for TinyML tasks, including context setup, task-specific goals, and error-handling protocols.\n\nor compiles locally. For instance, the system may execute\nPython code or compile an Arduino sketch, depending on the\nstage requirements. To illustrate, in the _data processing stage_,\nGPT-4o assists in automating preprocessing steps, including\ndata cleaning, normalization, and augmentation, ensuring that\nthe data is prepared for model training with minimal manual\nintervention. For _model conversion_ (optimization and quantization), the framework provides scripts and configurations\nto convert models into formats suitable for deployment on\nresource-constrained devices, such as TensorFlow Lite. Finally, in the deployment stage ( _sketch generation_ ), the LLM\ngenerates scripts tailored to the specific hardware and application requirements, including code for deploying models on\ndevices like the Arduino Nano 33 BLE. It is worth mentioning\nthat our work focuses specifically on representative stages of\nthe ML lifecycle that are unique to TinyML workflows due\nto the computing constraints and requirements of resourceconstrained devices. These include stages like model optimization and deployment, which necessitate additional operations\ncompared to a traditional ML lifecycle. We deliberately omit\nother stages, such as model training, as these remain largely\nidentical in both TinyML and traditional ML workflows. For\neach lifecycle stage, if an error arises during local execution,\nthe system triggers an iterative retry mechanism that engages\nthe LLM with specialized error-handling prompts. This iterative process continues, refining the output until successful\n\nexecution is achieved or until the maximum retry threshold is\nreached. The process concludes either with the generation of\nfinal artifacts, such as processed datasets, converted models,\nor Arduino sketches, or with termination if the iteration limit\nis exceeded.\nFig. 4 shows the interactions between the user, framework, LLM, device compiler, and target device during the\nsketch deployment stage, including the retry mechanisms\nthat ensure robust code generation and deployment. It is\nworth emphasizing that the framework includes a feedbackdriven retry mechanism: when an operation fails (e.g., due\nto compilation or deployment errors, the system parses the\nerror and updates the prompt accordingly. This process is\nhandled automatically, enabling the LLM to regenerate code\nthat better aligns with the hardware constraints or resolves the\nencountered issue—without manual prompt rewriting. Instead\nof displaying the entire prompt code, the snippet on the\nright provides a structured template summarizing the core\ncomponents of the prompt, including context, objectives, taskspecific instructions, and error handling. This template guides\nthe LLM in generating code that aligns with the hardware\nconstraints and application requirements.\n\nVI. CASE STUDY: FROM FRUIT TO FUNCTIONALITY\n\nWhen evaluating the implemented system, we aimed to\naddress three key questions (Q#):\n30\n20\n10\n0\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Data<br>Sketc<br>Mode<br>DP Er|Col8|Col9|(DP)<br>on (SG)<br>on (MC)|\n|---|---|---|---|---|---|---|---|---|---|\n|||||||Data<br>Sketc<br>Mode<br>DP E|Data<br>Sketc<br>Mode<br>DP E|Processing<br>h Generati<br>l Conversi<br> ror|(DP)<br> on (SG)<br> on (MC)|\n|||||||<br>SG Er|<br>SG Er|<br> ror||\n|||||||<br>SG Er|<br>SG Er|||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n\n0 20 40 60 80 100 120 140 160 180\n\nTime (Seconds)\n\nFig. 5: The scatter plot illustrates the relationship between time\nconsumption (in seconds) and token consumption across three\nTinyML stages: DP, MC, and SG. Successful and failed operations are distinguished by marker types. The accompanying\ntable summarizes average, minimum, and maximum execution\ntimes and token consumption for each stage, along with the\ntask completion success rate.\n\n**(Q1)** _**TASK COMPLETION SUCCESS:**_ _How reliably does_\n_the framework and the LLM successfully execute tasks_\n_in each developed lifecycle stage?_\n**(Q2)** _**EXECUTION TIME:**_ _What is the time required to com-_\n_plete each stage of the lifecycle, and how does this reflect_\n_the framework’s efficiency?_\n**(Q3)** _**OPERATIONAL COST:**_ _What are the monetary costs_\n_associated with recursive LLM API calls, and how do_\n_they balance against the benefits of automation?_\n\nThese dimensions are inherently interconnected but provide\ndifferent perspectives on the system’s performance. Success\nrate evaluates the system’s robustness and reliability, time\nconsumption measures efficiency, and monetary cost assesses\nthe practicality of the approach in resource-constrained environments.\nTo explore these questions, we conducted a detailed case\nstudy involving a fruit classification task—representative of\ncomputer vision workloads—on the Arduino Nano 33 BLE\nboard. This use case was chosen because it aligns with an\nofficially supported Arduino example, allowing us to validate\nour LLM-generated sketch against a reliable reference implementation. While simple in nature, it includes the typical\nlifecycle steps (data processing, quantization, and hardwarespecific code generation), and our framework is readily extensible to more complex datasets or embedded ML applications.\nFrom the ML model perspective, we employed a simple computer vision approach using a convolutional neural network\n(CNN) model for fruit classification. This model is designed to\nclassify different types of fruits (e.g., apples, bananas, oranges)\nbased on their color variations, leveraging the device’s built-in\nRGB color sensor.\n\n**Empirical Evaluation.** We conducted thirty full runs for\neach lifecycle stage under evaluation—data processing, 8bit model quantization and conversion, and sketch generation—using GPT-4o (GPT-4o-2024-08-06). Tokens, the\nsmallest units of text input processed by LLMs, served as a\nkey metric for evaluating resource consumption. To ensure\ncontrolled testing, the maximum number of iterations for\nresolving errors through the framework was capped at **five**\n(customizable), preventing excessive retries while allowing\nprompt adaptation to converge within practical bounds. A stage\nwas considered successful if it completed without LLM failure\nor compilation error within the retry limit. Execution time\nwas measured from the initial LLM query to the successful\ncompletion of the stage. Per-stage results were aggregated over\n30 runs, and we reported mean, range, and success rate for\neach stage to reflect both central tendency and variability. The\nimplementation of the framework spans approximately 1,578\nlines of Python code, covering the automation logic, prompt\nhandling, error parsing, and interaction with the LLM API\nand device toolchain (i.e., _Arduino Command Line Tool_ in\nthis case). It is worth highlighting, this end-to-end process\nintroduces no additional computational burden on the target\ndevice, as all LLM interactions and code generation occur\noff-device, and the resulting deployment artifacts are nearly\nidentical to those produced via standard TinyML workflows.\nAmong the three TinyML stages, _Sketch Generation (SG)_\nemerged as the most resource-intensive and least reliable.\nThis stage exhibited a success rate of only 36.7%, with\nsubstantial token consumption averaging 13,321 tokens (range:\n1,840–17,181). Execution times were similarly high, averaging 60.55 seconds (range: 7.73–87.92s). In contrast, _Data_\n_Processing (DP)_ showed moderate resource utilization with\nan average token consumption of 10,832 tokens (range:\n8,560–25,086) and mean execution time of 47.76 seconds\n(range: 32.58–155.93s), achieving a 90% success rate. The\n_Model INT8 Quantization & Conversion (MC)_ stage stood out\nas the most efficient and reliable, maintaining a 100% success\nrate with minimal resource consumption: an average of 689\ntokens (range: 545–3,949) and mean execution time of just\n6.09 seconds (range: 3.65–10.21s).\nFig. 5 highlights these performance variations across different TinyML stages, underscoring the specific challenges of the\nSG phase. The figure also highlights the relationship between\nexecution time and token consumption, revealing distinct and\nstage-specific patterns. MC clusters tightly in the lower-left\nquadrant (mean: 6.09s, 689 tokens), demonstrating consistent and efficient performance. This efficiency likely stems\nfrom well-defined input/output specifications and standardized\nTensorFlow Lite procedures. DP, on the other hand, exhibits\nmoderate dispersion. A bimodal distribution is evident, with\nsuccessful runs consuming fewer resources compared to failed\nones, suggesting that failures often result from complex\nedge cases requiring significantly more computational effort.\nSketch Generation presents the most scattered distribution,\nreflecting substantial variability in resource requirements. Its\nunpredictability likely arises from the inherent complexity of\n120\n80\n40\n0\n\n0.4\n\n0.35\n\n0.3\n\n0.25\n\n0.2\n\n0.15\n\n0.1\n\n0.05\n|Execution Time Operational Cost|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|\n|---|---|---|---|---|---|---|---|---|---|---|\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n|Successful<br>Unsuccessful<br>DP|Successful<br>Unsuccessful<br>DP|Successful<br>Unsuccessful<br>DP|Successful<br>Unsuccessful<br>DP|Successful<br>Unsuccessful<br>DP|Successful<br>Unsuccessful<br>MC|Successful<br>Unsuccessful<br>SG|Successful<br>Unsuccessful<br>SG|Successful<br>Unsuccessful<br>SG|Successful<br>Unsuccessful<br>SG|Successful<br>Unsuccessful<br>SG|\n\nFig. 6: Trade-offs and stage-specific resource consumption\nconcerning execution time (seconds) and operational cost\n(USD) for successful and unsuccessful runs across DP, MC,\nand SG stages.\n\ntranslating comprehensive specifications into executable C++\ncode.\nThe SG stage’s high failure rate (63.3%) and resource\nconsumption variability indicate significant challenges in automated code generation. Failed operations are not simply binary\noutcomes but involve prolonged computational processes that\nexhaust resources before declaring failure. This is in contrast\nto DP and MC, which demonstrate relative stability and\nefficiency.\nIn summary, while DP and MC stages are relatively robust\nand efficient, SG remains a bottleneck due to its lower success rate, higher resource consumption, and variability. These\nfindings highlight the need for further optimization in the\nautomated generation of deployment sketches.\nTo provide deeper insights into the trade-offs across lifecycle stages, we analyzed the relationship between execution\ntime and operational cost for both successful and unsuccessful\nruns (Fig. 6). We observed that unsuccessful runs consistently\nexhibit a significant increase in execution time compared to\nsuccessful ones, highlighting the resource-intensive nature of\nerror-prone processes. This effect is particularly evident in\nstages like Sketch Generation, where iterative refinements\nexacerbate execution time variability. In contrast, the operational cost remains relatively stable across both successful and\nunsuccessful runs, with negligible variance. While this cost\nmay appear minimal at a per-run level, it is critical to consider\nthe cumulative impact in large-scale IoT deployments.\nIn real-world scenarios involving thousands of devices or\nfrequent lifecycle operations—such as periodic updates for\npredictive maintenance or environmental monitoring—these\ncosts can accumulate significantly. Moreover, the high variance\nin execution time introduces unpredictability, which could\npose challenges for time-sensitive IoT applications. This analysis underscore the need for more robust and efficient mechanisms, particularly for error-prone stages, to ensure scalability\nand cost-effectiveness in IoT-centric TinyML workflows.\n\n**Demo.** To illustrate the practical application of the proposed\nframework, we provide two video demonstrations. The first\nvideo [1] highlights the automation of the model quantization\nand conversion stage, where ML models are optimized for\nresource-constrained devices and converted into a TensorFlow\nLite-compatible format. The second video [2] demonstrates the\nautomated generation of C++ sketch code for the targeted\nArduino, enabling the execution of the converted model.\n\nVII. CHALLENGES AND HORIZONS\n\nOur work highlights the potential of LLMs to enhance\nthe TinyML lifecycle while uncovering key challenges that\nmust be addressed. This section explores avenues for further\ndevelopment identified during our research.\n**Enhancing Error Resilience.** One significant challenge lies\nin the system’s limited ability to self-correct, especially in\nmulti-step processes where errors in early stages propagate\nand compound. A promising direction involves considering\nLLM-based multi-agent systems, combining smaller, locally\ndeployed LLMs with more capable, cloud-based models.\nLocal LLMs could handle error detection and validation,\nensuring outputs adhere to expected formats before engaging larger models for more complex tasks. Although performance–cost–bandwidth trade-offs should be evaluated on\na case-by-case basis, this approach can help balance privacy, efficiency, and cost, as local models reduce reliance on\ncostly cloud resources. However, computational constraints on\nTinyML devices make deploying sophisticated models locally\nchallenging, necessitating further optimization and adaptation.\nIn such cases, an alternative strategy could involve equipping\nthe framework, which effectively stands between the end\ndevice and the cloud, with a lightweight, embedded LLM\nacting as a local proxy. This would enable more responsive coordination and selective filtering, improving latency\nand efficiency in distributed inference workflows. In addition, the use of adaptive prompt templates, where prompt\nstructure evolves dynamically based on execution outcomes\nand feedback, may further enhance the system’s ability to\nself-correct. Such templates can prevent repeated failures by\nrefining LLM instructions contextually, improving reliability\nacross iterations.\n**Streamlining Resource-Intensive Processes.** Sketch generation emerged as a bottleneck due to high resource consumption and lower success rates. Enhancing prompt engineering\nand workflow design could address this, focusing on precise,\ncontext-aware prompts and iterative feedback loops to reduce\nerrors. As mentioned above, adaptive prompt templates represent a promising technique in this direction, enabling more\nreliable and tailored code generation through dynamic prompt\nevolution. Additionally, exploring domain-specific languages\n(DSLs) tailored for target platforms, such as Arduino, could\nstreamline code generation. As an example, DSLs inspired\nby symbolic programming languages, such as Prolog, have\n\n[1https://youtu.be/KnJ5m78x X8](https://youtu.be/KnJ5m78x_X8)\n[2https://youtu.be/Ojpsb5Wnnl8](https://youtu.be/Ojpsb5Wnnl8)\nbeen investigated for improving reasoning and control logic in\nconstrained environments, offering an avenue to complement\nLLM-based generation with more structured and explainable\nconstructs. Finally, hybrid approaches that combine LLMgenerated drafts with traditional compilers and manual developer refinement may further improve reliability and efficiency,\nespecially for complex or resource-intensive tasks.\n**Adaptive Task Allocation for Cost Efficiency.** Monetary\ncosts associated with API calls become significant at scale,\nparticularly in dynamic IoT deployments. Adaptive LLM\nrouting offers a solution, directing simpler tasks to smaller,\ncost-effective models while reserving more complex queries\nfor larger models. For instance, tasks such as data processing\nor model conversion—which in our evaluation showed higher\nsuccess rates—can be routed to LLMs with lower-cost APIs.\nThis introduces a trade-off: slightly lower reliability may\nrequire more iterations, but can yield meaningful cost savings.\nThis strategy can extend to specialized LLMs for TinyML,\nenabling dynamic updates or functionality shifts, such as\nadding new features or reconfiguring device applications. For\nexample, if an application initially performs object detection\nbut later requires object tracking, the framework can route\nthe request to a model more specialized in temporal or visual\nsequence tasks. By acting as a capability discovery system, the\nframework can negotiate with specialized LLMs to identify the\nmost efficient solution, optimizing cost and performance while\nmaintaining adaptability for evolving TinyML requirements.\n\nVIII. CONCLUSIONS: REALITY, ILLUSION, OR\nOPPORTUNITY?\n\nIn this paper, we explored the integration of LLMs into the\nTinyML lifecycle, investigating their potential to automate and\nstreamline key stages through the development of a lifecycle\nmiddleware framework tested through a practical IoT development scenario involving a classification use case. Our findings\nreveal both the promise and challenges of this approach. On\nthe positive side, LLMs demonstrate the ability to improve\ndevelopment efficiency, as seen in tasks like model quantization and data preprocessing. However, limitations remain:\nreliability issues in complex tasks such as sketch generation,\nconstrained generalizability across hardware platforms, and\nsignificant resource demands in terms of computation and cost.\nThese challenges highlight the need for further refinement, including enhanced prompt engineering, specialized model finetuning, and the development of advanced symbolic reasoning\nmechanisms to improve error correction, iterative refinement,\nand reasoning capabilities. Despite these hurdles, the **opportu-**\n**nity** for transformation is clear. By addressing these challenges\nand expanding the framework to include additional lifecycle\nstages and more diverse hardware, LLMs can become a cornerstone of TinyML automation. This work lays the groundwork\nfor leveraging LLMs alongside traditional tools, paving the\nway for more efficient, scalable, and accessible embedded\nIoT ML workflows. Through the development of this end-toend framework for automating core TinyML lifecycle stages\nusing LLMs, we contribute a novel methodology that reduces\n\ndeveloper effort and highlights the feasibility of language\nmodel–driven embedded ML workflows. Looking forward,\nopen questions remain about the long-term generalizability of\nLLM-generated code across highly heterogeneous hardware\nplatforms, and how such models can reliably adapt to evolving application needs with minimal supervision. Addressing\nthese questions may define the next phase of LLM-enabled\nintelligence at the edge.\n\nIX. ACKNOWLEDGEMENTS\n\nThis work was supported by the _Digital Twinning of Per-_\n_sonal Area Networks for Optimized Sensing and Communica-_\n_tion_ project through the Business Finland 6G Bridge Program\n(8782/31/2022).\n\nREFERENCES\n\n[1] Y. Abadade, A. Temouden, H. Bamoumen, N. Benamar, Y. Chtouki,\nand A. S. Hafid, “A Comprehensive Survey on TinyML,” _IEEE_\n_Access_, vol. 11, pp. 96 892–96 922, 2023. [Online]. Available:\n[https://ieeexplore.ieee.org/document/10177729/](https://ieeexplore.ieee.org/document/10177729/)\n\n[2] L. Capogrosso, F. Cunico, D. S. Cheng, F. Fummi, and M. Cristani,\n“A Machine Learning-Oriented Survey on Tiny Machine Learning,”\n_IEEE Access_, vol. 12, pp. 23 406–23 426, 2024. [Online]. Available:\n[https://ieeexplore.ieee.org/document/10433185/](https://ieeexplore.ieee.org/document/10433185/)\n\n[3] S. Sai, M. Kanadia, and V. Chamola, “Empowering iot with generative\nai: Applications, case studies, and limitations,” _IEEE Internet of Things_\n_Magazine_, vol. 7, no. 3, pp. 38–43, 2024.\n\n[4] R. David, J. Duke, A. Jain, V. J. Reddi, N. Jeffries, J. Li, N. Kreeger,\nI. Nappier, M. Natraj, S. Regev, R. Rhodes, T. Wang, and P. Warden,\n“TensorFlow Lite Micro: Embedded Machine Learning on TinyML\nSystems.”\n\n[5] S. Hymel, C. Banbury, D. Situnayake, A. Elium, C. Ward, M. Kelcey,\nM. Baaijens, M. Majchrzycki, J. Plunkett, D. Tischler, A. Grande,\nL. Moreau, D. Maslov, A. Beavis, J. Jongboom, and V. J. Reddi, “Edge\nImpulse: An MLOps Platform for Tiny Machine Learning,” Apr. 2023.\n\n[[Online]. Available: http://arxiv.org/abs/2212.03332](http://arxiv.org/abs/2212.03332)\n\n[6] P. Warden and D. Situnayake, _Tinyml: Machine learning with tensorflow_\n_lite on arduino and ultra-low-power microcontrollers_ . O’Reilly Media,\n2019.\n\n[7] J. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, and\nR. McHardy, “Challenges and Applications of Large Language\n[Models,” Jul. 2023. [Online]. Available: http://arxiv.org/abs/2307.10169](http://arxiv.org/abs/2307.10169)\n\n[8] P. Vaithilingam, T. Zhang, and E. L. Glassman, “Expectation vs.\nExperience: Evaluating the Usability of Code Generation Tools\nPowered by Large Language Models,” in _CHI_ _Conference_ _on_\n_Human Factors in Computing Systems Extended Abstracts_ . New\nOrleans LA USA: ACM, Apr. 2022, pp. 1–7. [Online]. Available:\n[https://dl.acm.org/doi/10.1145/3491101.3519665](https://dl.acm.org/doi/10.1145/3491101.3519665)\n\n[9] J. Seo, N. Zhang, and C. Rong, “Flexible and Secure Code\nDeployment in Federated Learning using Large Language Models:\nPrompt Engineering to Enhance Malicious Code Detection,” in _2023_\n_IEEE International Conference on Cloud Computing Technology and_\n_Science (CloudCom)_ . Naples, Italy: IEEE, Dec. 2023, pp. 341–349.\n\n[[Online]. Available: https://ieeexplore.ieee.org/document/10475813/](https://ieeexplore.ieee.org/document/10475813/)\n\n[10] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao,\n“ReAct: Synergizing Reasoning and Acting in Language Models,” Mar.\n[2023. [Online]. Available: http://arxiv.org/abs/2210.03629](http://arxiv.org/abs/2210.03629)\n\n[11] J. Wen, J. Nie, J. Kang, D. Niyato, H. Du, Y. Zhang, and M. Guizani,\n“From generative ai to generative internet of things: Fundamentals,\nframework, and outlooks,” _IEEE Internet of Things Magazine_, vol. 7,\nno. 3, pp. 30–37, 2024.\n\n[12] Z. Englhardt, R. Li, D. Nissanka, Z. Zhang, G. Narayanswamy, J. Breda,\nX. Liu, S. Patel, and V. Iyer, “Exploring and characterizing large\nlanguage models for embedded system development and debugging,”\nin _Extended Abstracts of the CHI Conference on Human Factors in_\n_Computing Systems_, 2024, pp. 1–9.\n[13] M. Choaib, M. Garouani, M. Bouneffa, and Y. Mohanna, “Iot sensor\nselection in cyber-physical systems: Leveraging large language models\nas recommender systems,” in _2024 10th International Conference on_\n_Control, Decision and Information Technologies (CoDIT)_ . IEEE, 2024,\npp. 2516–2519.\n\n[14] P. M. Sooriya Patabandige, S. A. O. Waskito, K. Li, K. J. Leow,\nS. Chakrabarty, and A. Varshney, “Poster: Rethinking embedded sensor\ndata processing and analysis with large language models,” in _Proceed-_\n_ings of the 21st Annual International Conference on Mobile Systems,_\n_Applications and Services_, 2023, pp. 561–562.\n\n[15] J. Wang, L. Yu, and X. Luo, “Llmif: Augmented large language model\nfor fuzzing iot devices,” in _2024 IEEE Symposium on Security and_\n_Privacy (SP)_ . IEEE Computer Society, 2024, pp. 196–196.\n\n**Guanghan Wu** (Student Member, IEEE) is a Research Assistant in the\nDepartment of Computer Science at the University of Helsinki, Finland. He\nearned his Master’s degree in Computer Science from the same university in\n2024. (guanghan.wu@helsinki.fi)\n\n**Sasu Tarkoma** (Senior Member, IEEE) is a Professor of Computer Science\nand Dean of the Faculty of Science at the University of Helsinki, Finland.\nHe is affiliated with the Helsinki Institute for Information Technology (HIIT)\nand the Finnish Center for AI (FCAI). He is the chairman of the Finnish\nScientific Advisory Board for Defence (MATINE). His research interests\ninclude Internet and distributed systems, IoT, mobile computing, and AI.\n(sasu.tarkoma@helsinki.fi)\n\n**Roberto Morabito** (Member, IEEE) is an Assistant Professor in the Communication Systems Department at EURECOM, France. His research focuses on\nnetworked AI systems, with particular attention to AI service provisioning\nand lifecycle management under computing and networking constraints.\nHe earned his PhD from Aalto University and has held positions at the\nUniversity of Helsinki, Princeton University, and Ericsson Research Finland.\n(roberto.morabito@eurecom.fr)"}
